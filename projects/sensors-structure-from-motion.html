<!DOCTYPE html5>
<html lang="en-us">
<script type="text/javascript">date_cre = new Date(2019, 6, 15);</script>
<head>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>
   <script>
       $("head").load("/src/head.html");
   </script>
</head>
  <body>
    <div include-html="/content/header.html" page-id="projects"></div>

    <section class="about-section section">
  		<div class="container">
  			<div class="row">
  				<div class="col-sm-12 col-md-3">
  					<div class="heading">
  						<h3><b>Structure From Motion in MATLAB</b></h3>
  						<h6 class="font-lite-black"><b>ELEE 4230 - Sensors and Transducers</b></h6>
              <h6>Project co-authors: <a href="#">Alexis Dominguez</a> and <a href="https://www.linkedin.com/in/sahera-bader-393108102/">Sahera Bader</a></h6>
  					</div>
  				</div><!-- col-sm-3 -->
  				<div class="col-sm-12 col-md-9">
            <p>The premise of this class project was to address some kind of engineering problem using sensors, which is fairly broad if you think about it. Unfortunately, our first idea for this project didn't work out (it's been so long, I don't even remember what it was), thanks to LabVIEW being a buggy mess. Instead, I found MATLAB had a structure from motion (SfM) package that seemed perfect for this project. Additionally, the MOCI satellite mission I worked on at the University of Georgia was primarily intended to do SfM from low-earth orbit.</p>
            <p>We used the back camera of an iPhone 7, which produces 4032x3024px images. The MATLAB SfM package includes a pipeline that allowed us to produce results with fairly minimal additional code.</p>
            <p>Pretty much every camera distorts the images it takes in some way. For the most part, this is just an artifact of the optics, because rays that come in either closer to the edge of the lens, or at large angles from the optical axis, will not focus in the same place on a flat CMOS sensor. Without a detailed dicussion of the underlying SfM technology (with I don't pretend to fully understand), we prefer to work with images that are a true 2D representation of a 3D object, without the camera distortion. To remove the distortion, we use a set of calibration images (a black and white checkerboard) with some properties that we know, and then determine the skew and distortion from the camera. Understanding the characteristics of the camera allows us to "undo" the distortion so we can get closer to this 2D representation as an input for the SfM algorithm.</p>
            <p>With an undistorted image, we then attempt to find the corners and edges of features in the image. The idea is that if we can find the same set of feature edges in two images (i.e. matching them), we gain some understanding of the 3D geometry in the images. As of writing, being able to do this well, i.e. with some degree of accuracy and speed, is an active area of research. That is to say, I don't totally understand how it works, nor do I know how to do it well.</p>
            <p>Given a set of matched points we can determine where these points are in 3D space, and with some interpolation we can create a dense reconstruction of the features in a point cloud. The hope is that the point cloud contains a set of points in 3D that accurately describe where those points are on the actual object. We would look to see if primitive shapes or sharp color gradients are faithfully represented in the point cloud to determine the quality of the product.</p>
            <p>In the end, I'm inclined to say that the results look fairly good (at least much better than I expected them to). If you squint, you can kind of see where the NASA logo on the mug is, the handle, and the red inside. This was about the best we were able to do, trying a combination of imaging angles and lighting.</p>
            <p>If I were to redo this project today, I'd love to try using more images to aid in the reconstruction. Given the fairly primitive shape of a mug (being essentially a cylinder), the results we got showed some clear dependence on color for extracting features, so I would probably use some other object with more distinct geometric features. As far as I'm aware, there is a variety of ways to do feature matching and reconstruction, so it might be interesting to see how different methods compare on the same dataset.</p>
  				</div><!-- col-sm-9 -->
  			</div><!-- row -->
  		</div><!-- container -->
  	</section><!-- about-section -->


  </body>
  <script src="/src/collapsible.js"></script>

</html>
